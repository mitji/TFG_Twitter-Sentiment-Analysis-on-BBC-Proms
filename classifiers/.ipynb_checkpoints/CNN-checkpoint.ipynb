{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN implementation for Twitter Sentiment Analysis\n",
    "\n",
    "This is a Python implementation of the CNN proposed by Yoon Kim in https://arxiv.org/pdf/1408.5882.pdf.\n",
    "\n",
    "The implementation is an adaption of the implementation proposed in https://github.com/bentrevett/pytorch-sentiment-analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext.vocab as vocab\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from torchnlp.word_to_vector import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE CLASS INSTANCES\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "fields = {'tokens.token': ('tokens', TEXT),\n",
    "        'sentiment': ('sentiment', LABEL)}\n",
    "\n",
    "train_data, val_data, test_data = data.TabularDataset.splits(\n",
    "    path=\"../scripts\", train=\"train.jsonl\",\n",
    "    validation=\"valid.jsonl\", test=\"test.jsonl\",\n",
    "    format=\"json\",\n",
    "    fields=fields\n",
    ")\n",
    "\n",
    "# print(vars(train_data[0]))\n",
    "\n",
    "# BUILD VOCABULARY\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "\n",
    "\n",
    "# --- LOADING EMBEDDINGS FILE --- #\n",
    "# Glove 200dim twitter pre-trained word embeddings\n",
    "glove_embeddings = vocab.Vectors(name = '../../data/glove.twitter.27B/glove.twitter.27B.200d.txt',\n",
    "                                  cache = 'glove_embeddings',\n",
    "                                  unk_init = torch.Tensor.normal_)\n",
    "\n",
    "TEXT.build_vocab(train_data, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = glove_embeddings)\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "# check labels\n",
    "# print(LABEL.vocab.stoi)\n",
    "\n",
    "# CREATE ITERATORS\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, val_data, test_data),\n",
    "    sort = False, #sort by s attribute (quote)\n",
    "    batch_size = BATCH_SIZE, \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define network architecture and forward path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''Define network architecture and forward path'''\n",
    "    def __init__(self, vocab_size, vector_size, \n",
    "                 n_filters, filter_sizes, output_dim,\n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define word embeddings settings from the INPUT words\n",
    "        self.embedding = nn.Embedding(vocab_size, vector_size, padding_idx = pad_idx)\n",
    "        \n",
    "        # Specify CONVOLUTIONS with filters of different sizes (fs)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1,\n",
    "                                              out_channels = n_filters,\n",
    "                                              kernel_size = (fs, vector_size))\n",
    "                                    for fs in filter_sizes])\n",
    "        \n",
    "        # Add a FULLY CONNECTED LAYER for the final classification\n",
    "        self.linear = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        # Drop some of the nodes to increase robustness in training\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        '''Forward path of the network'''\n",
    "        \n",
    "        '''We have to unsqueeze our tensor to create a channel dimension value that the Conv2d needs'''\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        text = text.permute(1, 0)\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        # Get word embeddings\n",
    "        embedded  = self.embedding(text)\n",
    "        \n",
    "        # embedded = [batch size, sent len, embedding dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        # embedded = [batch size, 1, sent len, embedding dim]\n",
    "        \n",
    "        # Perform convolutions and apply activation functions\n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        #each conv result = [batch size, n_filters, sent len - filter_size + 1]\n",
    "        \n",
    "        # Add pooling layer to reduce dimensionality\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #each pool result(for each filter size group) = [batch size, n_filters]\n",
    "        \n",
    "        # Concatenate pooled outputs into a feature map, add the Dropout layer\n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "        #cat = torch.cat(pooled, dim = 1)\n",
    "        #cat = [batch size, n_filters*len(filter_sizes)]\n",
    "        \n",
    "        # Pass the feature map with dropout to the fully connected layer\n",
    "        return self.linear(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET NETWORK PARAMETERS\n",
    "# Vocab size\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "\n",
    "# Vector size (lower-dimensional representation of each word)\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "# Number of filters (for each filter SIZE)\n",
    "NUM_FILTERS = 300\n",
    "\n",
    "# N-grams that we want to analize using filters\n",
    "FILTER_SIZES = [4,4,4]\n",
    "\n",
    "# Output of the linear layer (probability of a negative review)\n",
    "OUTPUT_DIM = len(LABEL.vocab)\n",
    "\n",
    "# Proportion of units to drop\n",
    "DROPOUT = 0.5\n",
    "\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "# CREATE MODEL INSTANCE\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, NUM_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "TEXT.vocab.vectors.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our embedding layer to use the vocabulary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PRE-TRAINED WORD EMBEDDINGS\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we initialize the unknown and padding token embeddings to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZERO THE INITIAL WEIGHTS OF THE UNNKNOWN AND PADDING TOKENS\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "# The string token used as padding is '<pad>' by default\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use **CrossEntropyLoss** because is used when the examples belong to one of C classes, whereas BCEWithLogitsLoss is used when our examples exlusively belong to only 2 classes (0 and 1) and is also used in the case where our examples belong to between 0 and C classes (aka multilabel classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET MODEL PARAMETERS\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# place model and loss function on the GPU\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# HELPER FUNCTIONS (accuracy, epoch time)\n",
    "\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "def compute_precision(preds, y):\n",
    "    ''' Precision = TP  / FP + TP '''\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    incorrect = max_preds.squeeze(1).ne(y)\n",
    "    correct.sum() / float(correct.sum() + incorrect.sum())\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LOOP FUNCTION\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.tokens)\n",
    "        \n",
    "        loss = criterion(predictions, batch.sentiment)\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, batch.sentiment)\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        # Optimize weights\n",
    "        optimizer.step()\n",
    "        # Record accuracy and loss\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "# EVALUATION LOOP FUNCTION\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "def evaluate_validation(model, iterator, criterion):\n",
    "    \n",
    "    global_loss = 0\n",
    "    global_acc = 0\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    # Evaluation mode, turn off dropout while evaluating\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tokens)\n",
    "            \n",
    "            loss = criterion(predictions, batch.sentiment)\n",
    "            acc = categorical_accuracy(predictions, batch.sentiment)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    global_loss = epoch_loss / len(iterator)\n",
    "    global_acc = epoch_acc / len(iterator)\n",
    "    \n",
    "    return global_loss, global_acc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def evaluate_test(model, iterator, criterion, avg_method):\n",
    "    \n",
    "    global_loss = []\n",
    "    global_acc = []\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    # Evaluation mode, turn off dropout while evaluating\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.tokens)\n",
    "            \n",
    "            true_labels = [*true_labels, *batch.sentiment.numpy().tolist()]\n",
    "            predicted_labels = [*predicted_labels, *predictions.argmax(dim = 1).numpy().tolist()]\n",
    "            \n",
    "            loss = criterion(predictions, batch.sentiment)\n",
    "            acc = categorical_accuracy(predictions, batch.sentiment)\n",
    "            \n",
    "            global_loss.append(loss.item())\n",
    "            global_acc.append(acc.item()*100)\n",
    "\n",
    "    avgRec = recall_score(true_labels, predicted_labels, average=avg_method)*100\n",
    "    precision = precision_score(true_labels, predicted_labels, average=avg_method)*100\n",
    "\n",
    "    # binary classif: \n",
    "    f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "    fq = np.mean(f1[1], f1[2])*100\n",
    "    # create confusion matrix\n",
    "    cm_nsamples = confusion_matrix(true_labels, predicted_labels, labels=[0,1,2])\n",
    "    cm = confusion_matrix(true_labels, predicted_labels, labels=[0,1,2], normalize=\"true\")\n",
    "    nrows, ncols = cm.shape\n",
    "    \n",
    "    #cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    #cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    index = ['positive','neutral','negative']  \n",
    "    columns = ['positive','neutral', 'negative']\n",
    "    # transform to df for easier ploting\n",
    "    cm_df = pd.DataFrame(cm,columns,index)\n",
    "    cm_df.index.name = 'Actual labels'\n",
    "    cm_df.columns.name = 'Predicted labels'\n",
    "    annot = np.empty_like(cm).astype(str)    \n",
    "    nsamples = [2375, 5937, 3972] # [positive, neutral, negative]\n",
    "    for i in range(nrows):\n",
    "        count = 0\n",
    "        for j in range(ncols):\n",
    "            p = cm[i, j]*100\n",
    "            if i == j:\n",
    "                s = np.sum(cm_nsamples[i])\n",
    "                annot[i, j] = '%.2f%%\\n%d/%d' % (p, cm_nsamples[i][j], s)\n",
    "            elif cm_nsamples[i][j] == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.2f%%\\n%d' % (p, cm_nsamples[i][j])\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.title(f'Confusion Matrix\\nAvgRec: {avgRec:.3f}% | F1: {f1:.3f}% | Acc: {np.mean(global_acc): .3f}% \\nConfusion matrix')\n",
    "    plot_cm = sns.heatmap(cm_df, annot=annot, fmt='', cmap='rocket_r', vmin=0, vmax=1)\n",
    "    fig = plot_cm.get_figure()\n",
    "    fig.savefig('baseline.png') \n",
    "    return np.mean(global_loss), np.mean(global_acc), precision, avgRec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN LOOP\n",
    "\n",
    "N_EPOCHS = 5\n",
    "best_valid_loss = float('inf')\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "tr_loss = []\n",
    "tr_acc = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_validation(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save training metrics\n",
    "    val_loss.append(valid_loss)\n",
    "    val_acc.append(valid_acc)\n",
    "    tr_loss.append(train_loss)\n",
    "    tr_acc.append(train_acc)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), './model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.3f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and loss plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot accuracy and loss\n",
    "fig, axis = plt.subplots(1, 2, figsize=(15,5))\n",
    "axis[0].plot(val_loss, label='Validation loss')\n",
    "axis[0].plot(tr_loss, label='Training loss')\n",
    "axis[0].set_title('Losses')\n",
    "axis[0].set_xlabel('Epoch')\n",
    "axis[0].set_ylabel('Loss')\n",
    "axis[0].legend()\n",
    "axis[1].plot(val_acc, label='Validation accuracy')\n",
    "axis[1].plot(tr_acc, label='Training accuracy')\n",
    "axis[1].set_title('Accuracies')\n",
    "axis[1].set_xlabel('Epoch')\n",
    "axis[1].set_ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('./model.pt'))\n",
    "\n",
    "test_loss, test_acc, test_prec, test_recall, test_f1 = evaluate_test(model, test_iterator, criterion, 'macro')\n",
    "\n",
    "print(f'Macro avg. recall: {test_recall:.2f}% | Macro avg. precision: {test_prec:.2f}%')\n",
    "print(f'Macro avg f1: {test_f1:.2f}%')\n",
    "print(f'Accuracy: {test_acc:.2f}% | Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "model.load_state_dict(torch.load('./models/model.pt'))\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = sentence.split(' ')\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor)\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
    "    return max_preds.item()\n",
    "\n",
    "# english stopwords would be removed\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "all_stopwords = np.unique(np.concatenate((list(nltk_stopwords), list(spacy_stopwords)), axis=None))\n",
    "\n",
    "# read json file\n",
    "import json\n",
    "bbcproms = \"../scripts/tweets2014.json\"\n",
    "with open(bbcproms, 'r') as json_file:\n",
    "    proms_data = json.load(json_file)\n",
    "\n",
    "\n",
    "# predict class for each element & add sentiment to tweet\n",
    "classes = ['neutral', 'negative', 'positive']\n",
    "count = 0\n",
    "for el in proms_data['data']:\n",
    "    el['tokens'] = [token for token in el['tokens'] if token['token'] not in all_stopwords]\n",
    "    prediction = predict_class(model, el['cleaned_text'])\n",
    "    sentiment = classes[prediction]\n",
    "    el['sentiment'] = sentiment\n",
    "    count += 1\n",
    "    print(count)\n",
    "\n",
    "# create json with classified tweets\n",
    "output = '../scripts/bbcproms_2014_classified.json'\n",
    "with open(output, 'w') as output_file:\n",
    "    json.dump(proms_data, output_file, indent=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
