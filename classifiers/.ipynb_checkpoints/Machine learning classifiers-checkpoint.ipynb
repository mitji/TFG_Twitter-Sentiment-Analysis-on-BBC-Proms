{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References of similar ML implementations:\n",
    "\n",
    "https://github.com/tthustla/yet_another_tiwtter_sentiment_analysis_part1/blob/master/Yet_Another_Twitter_Sentiment_Analysis_part1-Copy1.ipynb\n",
    "\n",
    "https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\n",
    "\n",
    "https://sajalsharma.com/portfolio/sentiment_analysis_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# import training and test data\n",
    "training_file = '../scripts/train.json'\n",
    "valid_file = '../scripts/valid.json'\n",
    "test_file = '../scripts/test.json'\n",
    "\n",
    "with open(training_file, 'r') as json_file:\n",
    "    training = json.load(json_file)\n",
    "        \n",
    "with open(valid_file, 'r') as json_file:\n",
    "    valid = json.load(json_file)\n",
    "        \n",
    "with open(test_file, 'r') as json_file:\n",
    "    test = json.load(json_file)\n",
    "\n",
    "training_data = training['data']\n",
    "training_texts = [tweet['cleaned_text'] for tweet in training_data]\n",
    "training_labels = [tweet['sentiment'] for tweet in training_data]\n",
    "\n",
    "test_data = test['data']\n",
    "test_texts = [tweet['cleaned_text'] for tweet in test_data]\n",
    "test_labels = [tweet['sentiment'] for tweet in test_data]\n",
    "\n",
    "valid_data = valid['data']\n",
    "valid_texts = [tweet['cleaned_text'] for tweet in valid_data]\n",
    "valid_labels = [tweet['sentiment'] for tweet in valid_data]\n",
    "\n",
    "train_data = np.concatenate((training_data, valid_data), axis=None)\n",
    "train_texts = np.concatenate((training_texts, valid_texts), axis=None)\n",
    "train_labels = np.concatenate((training_labels, valid_labels), axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/senticon-en.json', 'r') as json_file:\n",
    "      lexicon = json.load(json_file)\n",
    "lexicon_pos = lexicon['positive']\n",
    "lexicon_neg = lexicon['negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual BoW like in https://sajalsharma.com/portfolio/sentiment_analysis_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stopwords would be removed\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#generate Bag of Words\n",
    "def generateBoW(data, ignoreStopwords):\n",
    "    bow = {}\n",
    "    for tweet in data:\n",
    "        for token in tweet['tokens']:\n",
    "            word = token['token']\n",
    "            if ignoreStopwords:\n",
    "                if word not in stopwords:\n",
    "                    bow[word] = bow.get(word,0) + 1\n",
    "            else:\n",
    "                bow[word] = bow.get(word,0) + 1\n",
    "    return bow\n",
    "\n",
    "bow_train = generateBoW(train_data, True)\n",
    "len(bow_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate manual feature extractors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# syntactical features\n",
    "'''\n",
    "the number of occurrences of nouns, adjectives and adverbs are added to the feature vector.\n",
    "the number of emphasis tokens present in the tweet is added to the feature space\n",
    "'''\n",
    "def get_syntactic_features(data):\n",
    "    syntatic_features_dicts = []\n",
    "    for tweet in data:\n",
    "        syntatic_dict = {}\n",
    "        syntatic_dict['nouns'] = 0\n",
    "        syntatic_dict['adjectives'] = 0\n",
    "        syntatic_dict['adverbs'] = 0\n",
    "        syntatic_dict['emphasis'] = 0\n",
    "        for token in tweet['tokens']:\n",
    "            if token['tag'] == 'NOUN':\n",
    "                syntatic_dict['nouns'] = syntatic_dict['nouns'] + 1\n",
    "            elif token['tag'] == 'ADJECTIVE':\n",
    "                syntatic_dict['adjectives'] = syntatic_dict['adjectives'] + 1\n",
    "            elif token['tag'] == 'ADVERB':\n",
    "                syntatic_dict['adverbs'] = syntatic_dict['adverbs'] + 1\n",
    "            elif token['tag'] == 'EMPHASIS':\n",
    "                syntatic_dict['emphasis'] = syntatic_dict['emphasis'] + 1\n",
    "        syntatic_features_dicts.append(syntatic_dict)\n",
    "    \n",
    "    return syntatic_features_dicts\n",
    "\n",
    "# lexicon features\n",
    "def get_lexicon_features(data):\n",
    "    lexicon_feature_dict = []\n",
    "    for tweet in data:\n",
    "        tweet_dict = {}\n",
    "        pos_pol = []\n",
    "        neg_pol = []\n",
    "        for token in tweet['tokens']:\n",
    "            word = token['token']\n",
    "            if word in lexicon_pos:\n",
    "                pos_pol.append(lexicon_pos[word])\n",
    "            elif word in lexicon_neg:\n",
    "                neg_pol.append(lexicon_neg[word])\n",
    "            \n",
    "            p = len(pos_pol)\n",
    "            n = len(neg_pol)\n",
    "            pos_pol = [float(el) for el in pos_pol]\n",
    "            neg_pol = [float(el) for el in neg_pol]\n",
    "            tweet_dict['avg-pos'] = np.round(np.mean(pos_pol),3) if p > 0 else 0 \n",
    "            tweet_dict['avg-neg'] = np.round(np.mean(neg_pol),3) if n > 0 else 0 \n",
    "            tweet_dict['last-pos'] = pos_pol[-1] if p > 0 else 0\n",
    "            tweet_dict['last-neg'] = neg_pol[-1] if n > 0 else 0\n",
    "            tweet_dict['max-pos'] = max(pos_pol) if p > 0 else 0\n",
    "            tweet_dict['max-neg'] = min(neg_pol) if n > 0 else 0\n",
    "            # TWEET POLARITY COMPUTATION\n",
    "            # firs condition: P > N\n",
    "            if p > n:\n",
    "                if n > 0:\n",
    "                    tweet_dict['polarity'] = np.round(1 - n/p,3)\n",
    "                else:\n",
    "                    tweet_dict['polarity'] = np.mean(pos_pol)\n",
    "            elif p < n:\n",
    "                if p > 0:\n",
    "                    tweet_dict['polarity'] = np.round(p/n - 1,3)\n",
    "                else:\n",
    "                    tweet_dict['polarity'] = np.mean(neg_pol)\n",
    "            elif p == n:\n",
    "                tweet_dict['polarity'] = 0\n",
    "        lexicon_feature_dict.append(tweet_dict)\n",
    "    \n",
    "    return lexicon_feature_dict\n",
    "            \n",
    "            \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract syntactical features\n",
    "train_synt_dict = get_syntactic_features(train_data)\n",
    "test_synt_dict = get_syntactic_features(test_data)\n",
    "# convert features dictionary to a sparse representation, so that they can be used by sklearn ML algorithms\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Convert feature dictionaries to sparse representations\n",
    "train_synt_features = vectorizer.fit_transform(train_synt_dict)\n",
    "test_synt_features = vectorizer.transform(test_synt_dict)\n",
    "#train_custom_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract ngram eatures\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=100000, ngram_range=(1,3))\n",
    "#vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Generate feature vectors\n",
    "train_ngram_features = vectorizer.fit_transform(train_texts)\n",
    "test_ngram_features = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract lexicon features\n",
    "train_lex_dict = get_lexicon_features(train_data)\n",
    "test_lex_dict = get_lexicon_features(test_data)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Convert feature dictionaries to sparse representations\n",
    "train_lex_features = vectorizer.fit_transform(train_lex_dict)\n",
    "test_lex_features = vectorizer.transform(test_lex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate feature dictionaries\n",
    "from scipy.sparse import hstack, vstack\n",
    "training_combined = hstack([train_ngram_features, train_synt_features, train_lex_features])\n",
    "test_combined = hstack([test_ngram_features, test_synt_features, test_lex_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#train_scaler = StandardScaler(with_mean=False)\n",
    "#scaled_features = train_scaler.fit_transform(train_ngram_features)\n",
    "classifier = SGDClassifier(\n",
    "    loss='log',\n",
    "        random_state=0, \n",
    "        learning_rate='constant',\n",
    "        eta0=0.02,\n",
    "        max_iter=300, \n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15\n",
    "    )\n",
    "classifier.fit(training_combined, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "#test_scaler = StandardScaler(with_mean=False)\n",
    "#test_scaled_features = test_scaler.fit_transform(test_ngram_features)\n",
    "#test_predictions = lr.predict(test_combined)\n",
    "test_predictions = classifier.predict(test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report = classification_report(test_labels, test_predictions, output_dict=True)\n",
    "# get metrics of interest\n",
    "avg_rec = test_report['macro avg']['recall']*100\n",
    "f1_neg = test_report['negative']['f1-score']\n",
    "f1_pos = test_report['positive']['f1-score']\n",
    "avg_f1 = np.mean([f1_neg, f1_pos])*100\n",
    "acc = test_report['accuracy']*100\n",
    "print(f'Macro avgRec: {avg_rec:.2f}%')\n",
    "print(f'Macro avgF1: {avg_f1:.2f}%')\n",
    "print(f'Acc: {acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
