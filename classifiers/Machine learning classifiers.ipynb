{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Sentiment Analysis\n",
    "### Tf-idf with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/tthustla/yet_another_tiwtter_sentiment_analysis_part1/blob/master/Yet_Another_Twitter_Sentiment_Analysis_part1-Copy1.ipynb\n",
    "\n",
    "also check https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4\n",
    "\n",
    "and https://sajalsharma.com/portfolio/sentiment_analysis_tweets to implement lexicon with ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import json\n",
    "# .txt file with text already pre-processed\n",
    "# sentiment<SPACE>tweet per line\n",
    "\n",
    "# import training and test data\n",
    "training_file = '../scripts/train_binary.json'\n",
    "valid_file = '../scripts/valid_binary.json'\n",
    "test_file = '../scripts/test_binary.json'\n",
    "\n",
    "with open(training_file, 'r') as json_file:\n",
    "    training = json.load(json_file)\n",
    "        \n",
    "with open(valid_file, 'r') as json_file:\n",
    "    valid = json.load(json_file)\n",
    "        \n",
    "with open(test_file, 'r') as json_file:\n",
    "    test = json.load(json_file)\n",
    "\n",
    "training_data = training['data']\n",
    "training_texts = [tweet['cleaned_text'] for tweet in training_data]\n",
    "training_labels = [tweet['sentiment'] for tweet in training_data]\n",
    "\n",
    "test_data = test['data']\n",
    "test_texts = [tweet['cleaned_text'] for tweet in test_data]\n",
    "test_labels = [tweet['sentiment'] for tweet in test_data]\n",
    "\n",
    "valid_data = valid['data']\n",
    "valid_texts = [tweet['cleaned_text'] for tweet in valid_data]\n",
    "valid_labels = [tweet['sentiment'] for tweet in valid_data]\n",
    "\n",
    "train_data = np.concatenate((training_data, valid_data), axis=None)\n",
    "train_texts = np.concatenate((training_texts, valid_texts), axis=None)\n",
    "train_labels = np.concatenate((training_labels, valid_labels), axis=None)\n",
    "# generate dataframes\n",
    "#columns = ['sentiment', 'color']\n",
    "#df = pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/senticon-en.json', 'r') as json_file:\n",
    "      lexicon = json.load(json_file)\n",
    "lexicon_pos = lexicon['positive']\n",
    "lexicon_neg = lexicon['negative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW like in https://sajalsharma.com/portfolio/sentiment_analysis_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english stopwords would be removed\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#generate Bag of Words\n",
    "def generateBoW(data, ignoreStopwords):\n",
    "    bow = {}\n",
    "    for tweet in data:\n",
    "        for token in tweet['tokens']:\n",
    "            word = token['token']\n",
    "            if ignoreStopwords:\n",
    "                if word not in stopwords:\n",
    "                    bow[word] = bow.get(word,0) + 1\n",
    "            else:\n",
    "                bow[word] = bow.get(word,0) + 1\n",
    "    return bow\n",
    "\n",
    "bow_train = generateBoW(train_data, True)\n",
    "len(bow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ngrams\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# compute up to 3-grams\n",
    "def extract_ngrams_from_dataset(data, ignoreStopwords):\n",
    "    ngrams_count = {n:dict() for n in range(1,4)}\n",
    "    for tweet in data:\n",
    "        if ignoreStopwords:\n",
    "            tokens = [el['token'] for el in tweet['tokens'] if el['token'] not in stopwords]\n",
    "        else:\n",
    "            tokens = [el['token'] for el in tweet['tokens']]\n",
    "        for n in range(1,4):\n",
    "            ngram = ngrams(tokens, n)\n",
    "            # now you have an n-gram you can do what ever you want\n",
    "            # yield ngram\n",
    "            # you can count them for your language model?\n",
    "            for item in list(ngram):\n",
    "                if len(list(item)) == 1:\n",
    "                    item = item[0]\n",
    "                elif len(list(item)) == 2:\n",
    "                    item = f'{item[0]}-{item[1]}'\n",
    "                elif len(list(item)) == 3:\n",
    "                    item = f'{item[0]}-{item[1]}-{item[2]}'\n",
    "                ngrams_count[n][item] = ngrams_count[n].get(item, 0) + 1\n",
    "    return ngrams_count\n",
    "            \n",
    "ngrams_list = extract_ngrams_from_dataset(train_data, True)\n",
    "all_ngrams = {**ngrams_list[1], **ngrams_list[2], **ngrams_list[3]}\n",
    "len(all_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ngrams_from_tokens(tokens):\n",
    "    ngrams_count = {n:dict() for n in range(1,4)}\n",
    "    for n in range(1,4):\n",
    "        ngram = ngrams(tokens, n)\n",
    "        for item in list(ngram):\n",
    "            if len(list(item)) == 1:\n",
    "                item = item[0]\n",
    "            elif len(list(item)) == 2:\n",
    "                item = f'{item[0]}-{item[1]}'\n",
    "            elif len(list(item)) == 3:\n",
    "                item = f'{item[0]}-{item[1]}-{item[2]}'\n",
    "            ngrams_count[n][item] = ngrams_count[n].get(item, 0) + 1\n",
    "    return ngrams_count\n",
    "\n",
    "# convert raw tweets to feature dictionaries\n",
    "def generateFeatureDicts(data, removeStopwords):\n",
    "    feature_dicts = []\n",
    "    for tweet in data:\n",
    "        # build feature dictionary for tweet, list of ngrams with count to 0\n",
    "        tweet_feature_dict = all_ngrams.fromkeys(all_ngrams, 0)\n",
    "        # get tokens\n",
    "        if removeStopwords:\n",
    "            tokens = [el['token'] for el in tweet['tokens'] if el['token'] not in stopwords]\n",
    "        else:\n",
    "            tokens = [el['token'] for el in tweet['tokens']]\n",
    "        \n",
    "        # get ngrams of tweet\n",
    "        tweet_ngrams_dicts = extract_ngrams_from_tokens(tokens)\n",
    "        # join the three dictionaries of ngrams\n",
    "        tweet_ngrams = {**tweet_ngrams_dicts[1], **tweet_ngrams_dicts[2], **tweet_ngrams_dicts[3]}\n",
    "        \n",
    "        for gram, count in tweet_ngrams.items():\n",
    "            tweet_feature_dict[gram] = count\n",
    "        \n",
    "        feature_dicts.append(tweet_feature_dict)\n",
    "        print('done!')\n",
    "        '''\n",
    "        for token in tweet['tokens']:\n",
    "            word = token['token']\n",
    "            if removeStopwords:\n",
    "                if word not in stopwords:\n",
    "                    tweet_feature_dict[word] = tweet_feature_dict.get(word,0) + 1\n",
    "            else:\n",
    "                tweet_feature_dict[word] = tweet_feature_dict.get(word,0) + 1\n",
    "        feature_dicts.append(tweet_feature_dict)\n",
    "        '''\n",
    "    return feature_dicts\n",
    "\n",
    "# syntactical features\n",
    "'''\n",
    "the number of occurrences of nouns, adjectives and adverbs are added to the feature vector.\n",
    "the number of emphasis tokens present in the tweet is added to the feature space\n",
    "'''\n",
    "def get_syntactic_features(data):\n",
    "    syntatic_features_dicts = []\n",
    "    for tweet in data:\n",
    "        syntatic_dict = {}\n",
    "        syntatic_dict['nouns'] = 0\n",
    "        syntatic_dict['adjectives'] = 0\n",
    "        syntatic_dict['adverbs'] = 0\n",
    "        syntatic_dict['emphasis'] = 0\n",
    "        for token in tweet['tokens']:\n",
    "            if token['tag'] == 'NOUN':\n",
    "                syntatic_dict['nouns'] = syntatic_dict['nouns'] + 1\n",
    "            elif token['tag'] == 'ADJECTIVE':\n",
    "                syntatic_dict['adjectives'] = syntatic_dict['adjectives'] + 1\n",
    "            elif token['tag'] == 'ADVERB':\n",
    "                syntatic_dict['adverbs'] = syntatic_dict['adverbs'] + 1\n",
    "            elif token['tag'] == 'EMPHASIS':\n",
    "                syntatic_dict['emphasis'] = syntatic_dict['emphasis'] + 1\n",
    "        syntatic_features_dicts.append(syntatic_dict)\n",
    "    \n",
    "    return syntatic_features_dicts\n",
    "\n",
    "# lexicon features\n",
    "def get_lexicon_features(data):\n",
    "    lexicon_feature_dict = []\n",
    "    for tweet in data:\n",
    "        tweet_dict = {}\n",
    "        pos_pol = []\n",
    "        neg_pol = []\n",
    "        for token in tweet['tokens']:\n",
    "            word = token['token']\n",
    "            if word in lexicon_pos:\n",
    "                pos_pol.append(lexicon_pos[word])\n",
    "            elif word in lexicon_neg:\n",
    "                neg_pol.append(lexicon_neg[word])\n",
    "            \n",
    "            p = len(pos_pol)\n",
    "            n = len(neg_pol)\n",
    "            pos_pol = [float(el) for el in pos_pol]\n",
    "            neg_pol = [float(el) for el in neg_pol]\n",
    "            tweet_dict['avg-pos'] = np.round(np.mean(pos_pol),3) if p > 0 else 0 \n",
    "            tweet_dict['avg-neg'] = np.round(np.mean(neg_pol),3) if n > 0 else 0 \n",
    "            tweet_dict['last-pos'] = pos_pol[-1] if p > 0 else 0\n",
    "            tweet_dict['last-neg'] = neg_pol[-1] if n > 0 else 0\n",
    "            tweet_dict['max-pos'] = max(pos_pol) if p > 0 else 0\n",
    "            tweet_dict['max-neg'] = min(neg_pol) if n > 0 else 0\n",
    "            # TWEET POLARITY COMPUTATION\n",
    "            # firs condition: P > N\n",
    "            if p > n:\n",
    "                if n > 0:\n",
    "                    tweet_dict['polarity'] = np.round(1 - n/p,3)\n",
    "                else:\n",
    "                    tweet_dict['polarity'] = np.mean(pos_pol)\n",
    "            elif p < n:\n",
    "                if p > 0:\n",
    "                    tweet_dict['polarity'] = np.round(p/n - 1,3)\n",
    "                else:\n",
    "                    tweet_dict['polarity'] = np.mean(neg_pol)\n",
    "            elif p == n:\n",
    "                tweet_dict['polarity'] = 0\n",
    "        lexicon_feature_dict.append(tweet_dict)\n",
    "    \n",
    "    return lexicon_feature_dict\n",
    "            \n",
    "            \n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "positive_words = opinion_lexicon.positive()\n",
    "negative_words = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_features = get_lexicon_features(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate syntactic feature dictionaries\n",
    "train_synt_dict = get_syntactic_features(train_data)\n",
    "test_synt_dict = get_syntactic_features(test_data)\n",
    "# convert features dictionary to a sparse representation, so that they can be used by sklearn ML algorithms\n",
    "# https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Convert feature dictionaries to sparse representations\n",
    "train_synt_features = vectorizer.fit_transform(train_synt_dict)\n",
    "test_synt_features = vectorizer.transform(test_synt_dict)\n",
    "#train_custom_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate ngrams feature dictionaries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords, max_features=100000, ngram_range=(1,3))\n",
    "#vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "\n",
    "# Generate feature vectors\n",
    "train_ngram_features = vectorizer.fit_transform(train_texts)\n",
    "test_ngram_features = vectorizer.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lex_dict = get_lexicon_features(train_data)\n",
    "test_lex_dict = get_lexicon_features(test_data)\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "# Convert feature dictionaries to sparse representations\n",
    "train_lex_features = vectorizer.fit_transform(train_lex_dict)\n",
    "test_lex_features = vectorizer.transform(test_lex_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack, vstack\n",
    "training_combined = hstack([train_ngram_features, train_synt_features, train_lex_features])\n",
    "test_combined = hstack([test_ngram_features, test_synt_features, test_lex_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#train_scaler = StandardScaler(with_mean=False)\n",
    "#scaled_features = train_scaler.fit_transform(train_ngram_features)\n",
    "classifier = SGDClassifier(\n",
    "    loss='log',\n",
    "        random_state=0, \n",
    "        learning_rate='constant',\n",
    "        eta0=0.02,\n",
    "        max_iter=300, \n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.15\n",
    "    )\n",
    "classifier.fit(training_combined, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "#test_scaler = StandardScaler(with_mean=False)\n",
    "#test_scaled_features = test_scaler.fit_transform(test_ngram_features)\n",
    "#test_predictions = lr.predict(test_combined)\n",
    "test_predictions = classifier.predict(test_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report = classification_report(test_labels, test_predictions, output_dict=True)\n",
    "# get metrics of interest\n",
    "avg_rec = test_report['macro avg']['recall']*100\n",
    "f1_neg = test_report['negative']['f1-score']\n",
    "f1_pos = test_report['positive']['f1-score']\n",
    "avg_f1 = np.mean([f1_neg, f1_pos])*100\n",
    "acc = test_report['accuracy']*100\n",
    "print(f'Macro avgRec: {avg_rec:.2f}%')\n",
    "print(f'Macro avgF1: {avg_f1:.2f}%')\n",
    "print(f'Acc: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lr = 'constant' -> 0.02, early stopping, valid_split = 15%\n",
    "best ngrams = (1,2,3)\n",
    "\n",
    "## Logistic regression\n",
    "**Syntactic features**<br>\n",
    "Macro avgRec: 39.994%<br>\n",
    "Macro avgF1: 29.770%<br>\n",
    "Acc: 44.554%\n",
    "\n",
    "**Bag of ngrams (1,2,3)**<br>\n",
    "Macro avgRec: 57.035%<br>\n",
    "Macro avgF1: 52.657%<br>\n",
    "Acc: 59.614%\n",
    "\n",
    "**Lexicon**<br>\n",
    "Macro avgRec: 39.40%<br>\n",
    "Macro avgF1: 18.79%<br>\n",
    "Acc: 46.93%\n",
    "\n",
    "**Syntactic + bag of ngrams + lexicon**<br>\n",
    "Macro avgRec: 60.64%<br>\n",
    "Macro avgF1: 58.26%<br>\n",
    "Acc: 60.09%\n",
    "\n",
    "## SVM\n",
    "**Syntactic features**<br>\n",
    "Macro avgRec: 40.557%<br>\n",
    "Macro avgF1: 31.716%<br>\n",
    "Acc: 44.863%\n",
    "\n",
    "**Bag of ngrams (1,2,3)**<br>\n",
    "Macro avgRec: 56.571%<br>\n",
    "Macro avgF1: 52.097%<br>\n",
    "Acc: 59.175%\n",
    "\n",
    "**Lexicon**\n",
    "Macro avgRec: 37.353%<br>\n",
    "Macro avgF1: 15.242%<br>\n",
    "Acc: 46.475%\n",
    "\n",
    "**Syntactic + bag of ngrams + lexicon**<br>\n",
    "Macro avgRec: 60.329%<br>\n",
    "Macro avgF1: 57.783%<br>\n",
    "Acc: 58.540%<br>\n",
    "lr = 'constant' -> 0.02, early stopping, valid_split = 15%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
